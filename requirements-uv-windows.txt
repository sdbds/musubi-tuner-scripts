# This file was autogenerated by uv via the following command:
#    uv pip compile requirements.txt -o requirements-uv-windows.txt --index-strategy unsafe-best-match --no-build-isolation -p 3.11
-e musubi-tuner/.
    # via -r requirements.txt
absl-py==2.3.1
    # via tensorboard
accelerate==1.12.0
    # via
    #   -r requirements.txt
    #   musubi-tuner
adam-mini==1.1.1
    # via -r requirements.txt
adv-optm==1.4.0
    # via -r requirements.txt
aiofiles==24.1.0
    # via gradio
annotated-doc==0.0.4
    # via fastapi
annotated-types==0.7.0
    # via pydantic
anyio==4.12.1
    # via
    #   gradio
    #   httpx
    #   starlette
ascii-magic==2.3.0
    # via -r requirements.txt
av==14.0.1
    # via
    #   -r requirements.txt
    #   musubi-tuner
bitsandbytes==0.49.1
    # via
    #   -r requirements.txt
    #   musubi-tuner
brotli==1.2.0
    # via gradio
certifi==2025.11.12
    # via
    #   httpcore
    #   httpx
    #   requests
    #   sentry-sdk
charset-normalizer==3.4.4
    # via requests
click==8.3.1
    # via
    #   typer
    #   uvicorn
    #   wandb
colorama==0.4.6
    # via
    #   ascii-magic
    #   click
    #   tqdm
contourpy==1.3.3
    # via matplotlib
cycler==0.12.1
    # via matplotlib
dadaptation==3.2
    # via -r requirements.txt
diffusers==0.33.1
    # via
    #   -r requirements.txt
    #   musubi-tuner
easydict==1.13
    # via
    #   -r requirements.txt
    #   musubi-tuner
editables==0.5
    # via -r requirements.txt
einops==0.8.1
    # via
    #   -r requirements.txt
    #   flash-attn
    #   musubi-tuner
fastapi==0.128.0
    # via gradio
ffmpy==1.0.0
    # via gradio
filelock==3.20.1
    # via
    #   diffusers
    #   huggingface-hub
    #   torch
    #   transformers
flash-attn @ https://github.com/sdbds/flash-attention-for-windows/releases/download/2.8.3/flash_attn-2.8.3+cu130torch2.10.0cxx11abiFALSEfullbackward-cp311-cp311-win_amd64.whl.whl
    # via -r requirements.txt
fonttools==4.61.1
    # via matplotlib
fsspec==2025.12.0
    # via
    #   gradio-client
    #   huggingface-hub
    #   torch
ftfy==6.3.1
    # via
    #   -r requirements.txt
    #   musubi-tuner
gitdb==4.0.12
    # via gitpython
gitpython==3.1.45
    # via wandb
gradio==6.3.0
    # via -r requirements.txt
gradio-client==2.0.3
    # via gradio
groovy==0.1.2
    # via gradio
grpcio==1.76.0
    # via tensorboard
h11==0.16.0
    # via
    #   httpcore
    #   uvicorn
hatchling==1.28.0
    # via -r requirements.txt
heavyball==2.2.2
    # via -r requirements.txt
hf-xet==1.2.0
    # via huggingface-hub
httpcore==1.0.9
    # via httpx
httpx==0.28.1
    # via
    #   gradio
    #   gradio-client
    #   safehttpx
huggingface-hub==0.36.0
    # via
    #   -r requirements.txt
    #   accelerate
    #   diffusers
    #   gradio
    #   gradio-client
    #   musubi-tuner
    #   tokenizers
    #   transformers
idna==3.11
    # via
    #   anyio
    #   httpx
    #   requests
importlib-metadata==8.7.0
    # via diffusers
jinja2==3.1.6
    # via
    #   gradio
    #   torch
kiwisolver==1.4.9
    # via matplotlib
markdown==3.10
    # via tensorboard
markdown-it-py==4.0.0
    # via rich
markupsafe==3.0.3
    # via
    #   gradio
    #   jinja2
    #   werkzeug
matplotlib==3.10.0
    # via -r requirements.txt
mdurl==0.1.2
    # via markdown-it-py
mpmath==1.3.0
    # via sympy
networkx==3.6.1
    # via torch
numpy==2.3.5
    # via
    #   accelerate
    #   bitsandbytes
    #   contourpy
    #   diffusers
    #   gradio
    #   heavyball
    #   matplotlib
    #   opencv-python
    #   pandas
    #   pytorch-optimizer
    #   tensorboard
    #   torchvision
    #   transformers
    #   xformers
opencv-python==4.10.0.84
    # via
    #   -r requirements.txt
    #   musubi-tuner
opt-einsum==3.4.0
    # via heavyball
orjson==3.11.5
    # via gradio
packaging==25.0
    # via
    #   accelerate
    #   bitsandbytes
    #   gradio
    #   gradio-client
    #   hatchling
    #   huggingface-hub
    #   matplotlib
    #   tensorboard
    #   transformers
    #   wandb
pandas==2.3.3
    # via gradio
pathspec==0.12.1
    # via hatchling
pillow==12.0.0
    # via
    #   -r requirements.txt
    #   ascii-magic
    #   diffusers
    #   gradio
    #   matplotlib
    #   musubi-tuner
    #   tensorboard
    #   torchvision
platformdirs==4.5.1
    # via wandb
pluggy==1.6.0
    # via hatchling
prodigy-plus-schedule-free==2.0.1
    # via -r requirements.txt
prodigyopt==1.1.2
    # via -r requirements.txt
prompt-toolkit==3.0.51
    # via -r requirements.txt
protobuf==6.33.2
    # via
    #   tensorboard
    #   wandb
psutil==7.1.3
    # via accelerate
pydantic==2.12.5
    # via
    #   fastapi
    #   gradio
    #   wandb
pydantic-core==2.41.5
    # via pydantic
pydub==0.25.1
    # via gradio
pygments==2.19.2
    # via rich
pyparsing==3.2.5
    # via matplotlib
python-dateutil==2.9.0.post0
    # via
    #   matplotlib
    #   pandas
python-multipart==0.0.21
    # via gradio
pytorch-optimizer==3.9.0
    # via -r requirements.txt
pytz==2025.2
    # via pandas
pyyaml==6.0.3
    # via
    #   accelerate
    #   gradio
    #   huggingface-hub
    #   transformers
    #   wandb
regex==2025.11.3
    # via
    #   diffusers
    #   transformers
requests==2.32.5
    # via
    #   diffusers
    #   huggingface-hub
    #   transformers
    #   wandb
rich==14.2.0
    # via typer
safehttpx==0.1.7
    # via gradio
safetensors==0.4.5
    # via
    #   -r requirements.txt
    #   accelerate
    #   diffusers
    #   musubi-tuner
    #   transformers
sageattention @ https://github.com/sdbds/SageAttention-for-windows/releases/download/torch2100%2Bcu130/sageattention-2.2.0+cu130torch2.10.0-cp311-cp311-win_amd64.whl.whl
    # via -r requirements.txt
schedulefree==1.4.1
    # via -r requirements.txt
semantic-version==2.10.0
    # via gradio
sentencepiece==0.2.1
    # via
    #   -r requirements.txt
    #   musubi-tuner
sentry-sdk==2.48.0
    # via wandb
setuptools==80.9.0
    # via tensorboard
shellingham==1.5.4
    # via typer
six==1.17.0
    # via python-dateutil
smmap==5.0.2
    # via gitdb
starlette==0.50.0
    # via
    #   fastapi
    #   gradio
sympy==1.14.0
    # via torch
tensorboard==2.20.0
    # via -r requirements.txt
tensorboard-data-server==0.7.2
    # via tensorboard
tokenizers==0.22.1
    # via transformers
toml==0.10.2
    # via
    #   -r requirements.txt
    #   musubi-tuner
tomlkit==0.13.3
    # via gradio
torch==2.10.0
    # via
    #   -r requirements.txt
    #   accelerate
    #   adv-optm
    #   bitsandbytes
    #   flash-attn
    #   heavyball
    #   prodigy-plus-schedule-free
    #   pytorch-optimizer
    #   schedulefree
    #   torch-optimi
    #   torchvision
    #   xformers
torch-optimi==0.3.2
    # via -r requirements.txt
torchvision==0.25.0
    # via -r requirements.txt
tqdm==4.67.1
    # via
    #   -r requirements.txt
    #   huggingface-hub
    #   musubi-tuner
    #   transformers
transformers==4.57.1
    # via
    #   -r requirements.txt
    #   musubi-tuner
triton-windows==3.6.0.post25
    # via -r requirements.txt
trove-classifiers==2025.12.1.14
    # via hatchling
typer==0.21.1
    # via gradio
typing-extensions==4.15.0
    # via
    #   anyio
    #   fastapi
    #   gradio
    #   gradio-client
    #   grpcio
    #   huggingface-hub
    #   pydantic
    #   pydantic-core
    #   schedulefree
    #   starlette
    #   torch
    #   typer
    #   typing-inspection
    #   wandb
typing-inspection==0.4.2
    # via pydantic
tzdata==2025.3
    # via pandas
urllib3==2.6.2
    # via
    #   requests
    #   sentry-sdk
uvicorn==0.40.0
    # via gradio
voluptuous==0.15.2
    # via
    #   -r requirements.txt
    #   musubi-tuner
wandb==0.23.1
    # via -r requirements.txt
wcwidth==0.2.14
    # via
    #   ftfy
    #   prompt-toolkit
werkzeug==3.1.4
    # via tensorboard
xformers==0.0.34
    # via -r requirements.txt
zipp==3.23.0
    # via importlib-metadata
